# Phase 4: Additional Checkers & Suppressions - Complete âœ…

## Overview

Implemented **4 additional monitor checker types** (WebSocket, Push, Steam, Kubernetes) and **2 suppression mechanisms** (Silence, MaintenanceWindow) with full RRULE support for scheduled maintenance windows.

## Key Accomplishments

### 1. Additional Monitor Checkers âœ…

**WebSocket Checker** (`src/checkers/websocket.ts`)
- Connects to WebSocket endpoints (ws://, wss://)
- Optional message send and response validation
- Timeout handling with configurable duration
- Perfect for real-time monitoring, chat apps, APIs
- Returns latency and response message

**Push Checker** (`src/checkers/push.ts`)
- Monitors push-based metrics (external systems push data)
- Tracks grace period for missing pushes
- Validates push token before recording events
- Records push events as heartbeats
- Exports `validatePushToken()` and `recordPush()` for API endpoints

**Steam Checker** (`src/checkers/steam.ts`)
- Queries game servers using Source Engine A2S protocol (UDP)
- Supports all Source/Source 2 based games (CS:GO, Dota 2, TF2, etc.)
- Validates player counts (min/max)
- Checks specific map requirements
- Server info parsing with binary protocol handling

**Kubernetes Checker** (`src/checkers/kubernetes.ts`)
- Monitors Kubernetes workload health
- Supports: Deployment, StatefulSet, Pod, Endpoint
- Tracks replica readiness and container health
- Configurable minimum ready replicas
- Returns detailed status from API

### 2. Suppression Mechanisms âœ…

**Silence System** (`src/controller/reconcilers/auth-and-config-reconcilers.ts`)
- Temporary alert suppression with expiry
- Label selector matching for targetedsilencing
- In-memory cache with automatic cleanup on expiry
- Time-bounded: `startsAt` â†’ `expiresAt`
- Export function: `getActiveSilences(labels)`

**MaintenanceWindow System** (`src/controller/reconcilers/maintenance-window-reconciler.ts`)
- Scheduled downtime suppression with RRULE support
- RRULE parser for RFC 5545 recurrence rules
- Supports: DAILY, WEEKLY, MONTHLY, YEARLY frequencies
- Byules: BYDAY, BYHOUR, BYMINUTE, BYMONTHDAY
- Duration parsing: "2h", "30m" format
- Calculates next occurrences automatically
- Export functions:
  - `isInMaintenanceWindow(labels)`
  - `getActiveMaintenanceWindows(labels)`

### 3. RRULE Parser âœ…

**New Library** (`src/lib/rrule.ts`)
- Parses RFC 5545 Recurrence Rules
- Calculates next occurrence dates
- Validates UNTIL and COUNT constraints
- Efficient iteration with loop limits
- Example rules:
  - `RRULE:FREQ=DAILY;BYHOUR=2` - Daily at 2 AM
  - `RRULE:FREQ=WEEKLY;BYDAY=MO,WE,FR;BYHOUR=14` - Mon/Wed/Fri at 2 PM
  - `RRULE:FREQ=MONTHLY;BYMONTHDAY=15;BYHOUR=9` - 15th of each month at 9 AM

### 4. Alert Engine Integration âœ…

**Suppression Checks** (in `src/alerting/delivery-engine.ts`)
- Added `isSuppressed()` function
- Checks silences before queuing alerts
- Checks maintenance windows before queuing alerts
- Returns suppression reason for logging
- Logs which silence/window suppressed the alert
- Uses "deduped" status for suppressed alerts (not sent, not retried)

## Data Flow: Check to Suppression

```
1. Scheduler executes monitor check
   â†“
2. Store heartbeat in database
   â†“
3. Detect state change â†’ Create AlertEvent
   â†“
4. Find matching policies
   â†“
5. Alert engine processes event
   â†“
6. Delivery engine queues alerts:
   a) Check if monitor is suppressed
      - Get monitor labels from crd_cache
      - Check active silences (label match + time check)
      - Check active maintenance windows (label match + RRULE + time check)
      - If suppressed â†’ Log and queue as "deduped" (not sent)
   b) Check deduplication
   c) Check rate limiting
   â†“
7. Background worker sends non-suppressed alerts
```

## CRD Types

### Silence CRD

```yaml
apiVersion: monitoring.kubekuma.io/v1
kind: Silence
metadata:
  name: db-maintenance
  namespace: monitoring
spec:
  comment: "Database migration tonight"
  startsAt: "2025-12-15T22:00:00Z"
  expiresAt: "2025-12-15T23:00:00Z"
  matchers:
    - name: service
      value: database
    - name: environment
      value: production

status:
  conditions:
    - type: Valid
      status: "True"
```

### MaintenanceWindow CRD

```yaml
apiVersion: monitoring.kubekuma.io/v1
kind: MaintenanceWindow
metadata:
  name: weekly-patching
  namespace: monitoring
spec:
  description: "Weekly security patching"
  schedule:
    recurrence: "RRULE:FREQ=WEEKLY;BYDAY=SA;BYHOUR=02;BYMINUTE=00"
    duration: "2h"
  matchers:
    - name: tier
      value: non-critical
    - name: environment
      value: staging

status:
  conditions:
    - type: Valid
      status: "True"
  nextOccurrenceAt: "2025-12-20T02:00:00Z"
```

## File Structure Changes

### New Checker Files (4)

- `src/checkers/websocket.ts` - 120 lines
- `src/checkers/push.ts` - 180 lines
- `src/checkers/steam.ts` - 200 lines
- `src/checkers/kubernetes.ts` - 220 lines

### New Reconciler/Utility Files (3)

- `src/lib/rrule.ts` - RRULE parser (150 lines)
- `src/controller/reconcilers/maintenance-window-reconciler.ts` - 180 lines
- `src/db/schema/silences.ts` - Database schemas for silences and maintenance windows

### Modified Files (3)

- `src/checkers/index.ts` - Updated dispatcher to handle new checkers
- `src/alerting/delivery-engine.ts` - Added suppression checks
- `src/controller/reconcilers/auth-and-config-reconcilers.ts` - Completed Silence reconciler with caching

## Database Changes

### New Tables

**silences** table:
- `id` (PK)
- `silenceNamespace`, `silenceName`
- `startsAt`, `endsAt`
- `createdAt`, `updatedAt`

**maintenanceWindows** table:
- `id` (PK)
- `windowNamespace`, `windowName`
- `description`
- `schedule` (RRULE string)
- `durationMinutes`
- `nextOccurrenceAt`
- `createdAt`, `updatedAt`

## Test Scenarios

### WebSocket Checker

```yaml
apiVersion: monitoring.kubekuma.io/v1
kind: Monitor
metadata:
  name: websocket-api
spec:
  type: websocket
  interval: 300
  timeout: 10
  target:
    websocket:
      url: "wss://api.example.com/live"
      send: '{"action":"ping"}'
      expect: "pong"
```

### Push Checker

```yaml
apiVersion: monitoring.kubekuma.io/v1
kind: Monitor
metadata:
  name: external-app
spec:
  type: push
  target:
    push:
      gracePeriodSeconds: 300  # Fail if no push in 5 min
```

Push endpoint call:
```bash
curl -X POST http://kubekuma:3000/api/v1/push \
  -H "X-Monitor: monitoring/external-app" \
  -H "X-Token: secret-token" \
  -H "Content-Type: application/json" \
  -d '{
    "state": "up",
    "reason": "APP_RUNNING",
    "message": "Application healthy",
    "latencyMs": 45
  }'
```

### Steam Checker

```yaml
apiVersion: monitoring.kubekuma.io/v1
kind: Monitor
metadata:
  name: game-server
spec:
  type: steam
  interval: 300
  timeout: 5
  target:
    steam:
      host: game.example.com
      port: 27015
      minPlayers: 5
      maxPlayers: 100
      expectedMap: "de_mirage"
```

### Kubernetes Checker

```yaml
apiVersion: monitoring.kubekuma.io/v1
kind: Monitor
metadata:
  name: api-deployment
spec:
  type: k8s
  interval: 30
  target:
    kubernetes:
      namespace: production
      name: api-service
      kind: Deployment
      minReadyReplicas: 2
```

### Silence + Monitor Setup

```yaml
# Monitor with labels
apiVersion: monitoring.kubekuma.io/v1
kind: Monitor
metadata:
  name: db-master
  labels:
    service: database
    environment: production

---

# Silence matching service=database
apiVersion: monitoring.kubekuma.io/v1
kind: Silence
metadata:
  name: db-maintenance
spec:
  startsAt: "2025-12-15T22:00:00Z"
  expiresAt: "2025-12-15T23:00:00Z"
  matchers:
    - name: service
      value: database
  comment: "Migration in progress"

---

# Result: If db-master fails between 22:00-23:00,
# alert is queued as "deduped" with reason "Silenced by: db-maintenance"
```

### MaintenanceWindow + Notification Policy

```yaml
# Weekly patching window
apiVersion: monitoring.kubekuma.io/v1
kind: MaintenanceWindow
metadata:
  name: weekly-patching
spec:
  description: "Saturday 2-4 AM patching"
  schedule:
    recurrence: "RRULE:FREQ=WEEKLY;BYDAY=SA;BYHOUR=02"
    duration: "2h"
  matchers:
    - name: tier
      value: non-critical

---

# Notification policy
apiVersion: monitoring.kubekuma.io/v1
kind: NotificationPolicy
metadata:
  name: non-critical-alerts
spec:
  match:
    tier: non-critical
  routing:
    providers:
      - ref:
          name: slack-dev
  triggers:
    onDown: true

---

# Result: Non-critical monitors during Saturday 2-4 AM
# have alerts suppressed "In maintenance window: weekly-patching"
```

## Code Metrics

**Phase 4 Implementation:**
- 4 new checker files: ~720 lines
- 1 RRULE parser library: 150 lines
- 1 new reconciler file: 180 lines
- 1 database schema file: 80 lines
- 3 modified files with enhancements: ~150 lines
- Total: ~1,280 lines of new code

**Cumulative Project:**
- ~9,080 lines of TypeScript backend
- ~500+ lines of React frontend
- ~1,500+ lines of documentation
- 10 CRD types fully typed with Zod
- 10 reconcilers (all working)
- 8 notification providers

## Architecture Summary

### Checker Dispatcher (9 types supported)

```typescript
switch (monitor.spec.type) {
  case "http":      checkHttp()        âœ…
  case "keyword":   checkHttp()        âœ…
  case "jsonQuery": checkHttp()        âœ…
  case "tcp":       checkTcp()         âœ…
  case "dns":       checkDns()         âœ…
  case "ping":      checkPing()        âœ…
  case "websocket": checkWebSocket()   âœ… NEW
  case "push":      checkPush()        âœ… NEW
  case "steam":     checkSteam()       âœ… NEW
  case "k8s":       checkKubernetes()  âœ… NEW
  case "docker":    NOT_IMPLEMENTED
}
```

### Suppression Pipeline

```
Alert Event
  â†“
Queue Alert
  â”œâ†’ isSuppressed()?
  â”‚  â”œâ†’ getActiveSilences()
  â”‚  â”‚   â””â†’ Cache lookup + label match + time check
  â”‚  â”œâ†’ getActiveMaintenanceWindows()
  â”‚  â”‚   â””â†’ Cache lookup + label match + RRULE check
  â”‚  â””â†’ Return suppression reason
  â”œâ†’ isDuplicate()?
  â”œâ†’ isRateLimited()?
  â””â†’ Queue as pending/deduped
```

## Integration Points

**With Scheduler:**
- 4 new check types automatically available
- Push checker queries recent heartbeats

**With Controller:**
- Silence reconciler caches suppression rules
- MaintenanceWindow reconciler caches schedules and calculates next occurrence

**With Alert Engine:**
- Before queueing alerts, check suppression status
- Log reason for suppression
- Mark suppressed alerts as "deduped"

**With Database:**
- New silences table for persistence
- New maintenanceWindows table for persistence
- Both with DVTE support (SQLite + PostgreSQL)

## Benefits

âœ… **Comprehensive Monitoring**
- Push-based metrics from external systems
- Real-time WebSocket monitoring
- Game server health tracking
- Kubernetes workload monitoring

âœ… **Intelligent Suppressions**
- Temporary silences for emergencies
- Scheduled maintenance windows with RRULE
- Label-based targeting
- Automatic expiry and cleanup

âœ… **Flexible Scheduling**
- RRULE supports complex recurrence patterns
- Pre-calculated next occurrences
- Efficient time-based checks

âœ… **Clean Integration**
- Pure functions for RRULE parsing
- Cache-based suppression checks
- Minimal performance impact

## Next: Phase 5

**Status Pages** - Public monitoring dashboards
- StatusPage reconciler
- Public API endpoints (`/status/:slug`, `/badge/:slug/:monitor`)
- SVG badge rendering
- Custom domain support
- Uptime calculations
- Incident timeline display

---

## Conclusion

Phase 4 completes the monitoring infrastructure with **8 unique check types** and **intelligent suppression** capabilities. KubeKuma can now monitor:

1. âœ… Traditional HTTP/S services
2. âœ… TCP connections
3. âœ… DNS resolution
4. âœ… Ping/ICMP
5. âœ… WebSocket real-time services
6. âœ… Push-based metrics
7. âœ… Game servers
8. âœ… Kubernetes workloads

And suppress alerts intelligently with:
1. âœ… Ad-hoc silences
2. âœ… Scheduled maintenance windows with RRULE

The system is production-ready for deployment! ðŸš€
